# Initial setup

We have decided to use Kubernetes on AWS Fargate to deploy this API.

## AWS kubernetes cluster setup

Although some familiarity with Kubernetes would help understand and make sense of the commands we run, this guide assumes minimal pre-requisites.

### Install command-line utilities

  - aws cli
    - Install and **configure** the cli from [official guide](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
 - [kubectl](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
 - [eksctl](https://github.com/weaveworks/eksctl#installation)

### Authenticate aws cli with ECR

Let's first initialize few env variables that we'll use frequently

```
CONTAINER_REPO=406746673604.dkr.ecr.us-east-1.amazonaws.com
CLUSTER_NAME=lcs-eks
AWS_REGION=us-east-1
STACK_NAME=eksctl-$CLUSTER_NAME-cluster
```

Now authenticate with the container repo

```
aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $CONTAINER_REPO
```

Initialize a couple more env variables

```
VPC_ID=$(aws cloudformation describe-stacks --stack-name "$STACK_NAME" | jq -r '[.Stacks[0].Outputs[] | {key: .OutputKey, value: .OutputValue}] | from_entries' | jq -r '.VPC')
AWS_ACCOUNT_ID=$(aws sts get-caller-identity | jq -r '.Account')
```

### Create a cluster in EKS

```
eksctl create cluster --name $CLUSTER_NAME --version 1.16 --fargate --region=$AWS_REGION --zones=us-east-1a,us-east-1b
```

  - `--zones` because it was picking some zones where Fargate pods weren't available
  - `--fargate` automatically creates a default Fargate profile in the cluster
  - Be patient, this command takes quite a while to finish (~10mins)

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs]$ eksctl create cluster --name lcs-eks --version 1.16 --fargate --region=us-east-1 --zones=us-east-1a,us-east-1b
[ℹ]  eksctl version 0.22.0
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.96.0/19
[ℹ]  using Kubernetes version 1.16
[ℹ]  creating EKS cluster "lcs-eks" in "us-east-1" region with Fargate profile
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=lcs-eks'
[ℹ]  CloudWatch logging will not be enabled for cluster "lcs-eks" in "us-east-1"
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=lcs-eks'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "lcs-eks" in "us-east-1"
[ℹ]  2 sequential tasks: { create cluster control plane "lcs-eks", create fargate profiles }
[ℹ]  building cluster stack "eksctl-lcs-eks-cluster"
[ℹ]  deploying stack "eksctl-lcs-eks-cluster"
[ℹ]  creating Fargate profile "fp-default" on EKS cluster "lcs-eks"
[ℹ]  created Fargate profile "fp-default" on EKS cluster "lcs-eks"
[ℹ]  "coredns" is now schedulable onto Fargate
[ℹ]  "coredns" is now scheduled onto Fargate
[ℹ]  "coredns" pods are now scheduled onto Fargate
[ℹ]  waiting for the control plane availability...
[✔]  saved kubeconfig as "/home/roy/.kube/config"
[ℹ]  no tasks
[✔]  all EKS cluster resources for "lcs-eks" have been created
[ℹ]  kubectl command should work with "/home/roy/.kube/config", try 'kubectl get nodes'
[✔]  EKS cluster "lcs-eks" in "us-east-1" region is ready
```
</details>


### Associate IAM with cluster

```
eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs]$ eksctl utils associate-iam-oidc-provider --cluster lcs-eks --approve
[ℹ]  eksctl version 0.22.0
[ℹ]  using region us-east-1
[ℹ]  will create IAM Open ID Connect provider for cluster "lcs-eks" in "us-east-1"
[✔]  created IAM Open ID Connect provider for cluster "lcs-eks" in "us-east-1"
```
</details>


### Create IAM Policy

```
aws iam create-policy --policy-name ALBIngressControllerIAMPolicy --policy-document file://kubernetes/alb-ingress-iam-policy.json
```

### Create cluster role and bind it

```
kubectl apply -f kubernetes/rbac-role.yaml
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl apply -f kubernetes/rbac-role.yaml
clusterrole.rbac.authorization.k8s.io/alb-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/alb-ingress-controller created
```
</details>


### Create IAM role and service account

```
eksctl create iamserviceaccount \
--name alb-ingress-controller \
--namespace kube-system \
--cluster $CLUSTER_NAME \
--attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/ALBIngressControllerIAMPolicy \
--approve
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ eksctl create iamserviceaccount \
> --name alb-ingress-controller \
> --namespace kube-system \
> --cluster $CLUSTER_NAME \
> --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/ALBIngressControllerIAMPolicy \
> --approve
[ℹ]  eksctl version 0.22.0
[ℹ]  using region us-east-1
[ℹ]  1 iamserviceaccount (kube-system/alb-ingress-controller) was included (based on the include/exclude rules)
[!]  serviceaccounts that exists in Kubernetes will be excluded, use --override-existing-serviceaccounts to override
[ℹ]  1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount "kube-system/alb-ingress-controller", create serviceaccount "kube-system/alb-ingress-controller" } }
[ℹ]  building iamserviceaccount stack "eksctl-lcs-eks-addon-iamserviceaccount-kube-system-alb-ingress-controller"
[ℹ]  deploying stack "eksctl-lcs-eks-addon-iamserviceaccount-kube-system-alb-ingress-controller"
[ℹ]  created serviceaccount "kube-system/alb-ingress-controller"
```
</details>


### Deploy Ingress controller

Edit the file `kubernetes/alb-ingress-controller.yaml` and change the cluster name to the one you chose.

```
kubectl apply -f kubernetes/alb-ingress-controller.yaml
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl apply -f kubernetes/alb-ingress-controller.yaml
deployment.apps/alb-ingress-controller created
```
</details>


### Create a deployment for the API

```
kubectl apply -f kubernetes/kusama-api-deployment.yaml
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl apply -f kubernetes/kusama-api-deployment.yaml
deployment.apps/kusama-api-deployment created
```
</details>


### Create a service for the API

```
kubectl apply -f kubernetes/kusama-api-service.yaml
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl apply -f kubernetes/kusama-api-service.yaml
service/kusama-api-service created
```
</details>


### Create AWS application load balances using Ingress

```
kubectl apply -f kubernetes/kusama-api-ingress.yaml
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl apply -f kubernetes/kusama-api-ingress.yaml
ingress.extensions/kusama-api-ingress created
```
</details>


Once created, you can get the endpoit of the load balancer with

```
kubectl get ingress kusama-api-ingress
```

<details>
<summary>Example output</summary>

```bash
[roy@miller-thinkpad:~/_code_/lcs/kusama-api]$ kubectl get ingress kusama-api-ingress
NAME                 HOSTS   ADDRESS                                                                  PORTS   AGE
kusama-api-ingress   *       a111111a-default-kusamaapi-xx11-1234567890.us-east-1.elb.amazonaws.com   80      98s

```
</details>


## Deploying new image to production as pods

### Building for production

```
COMMIT_HASH=$(git rev-parse --short HEAD) docker-compose -f docker-compose.prod.yaml build
```

### Push the image to container repo

```
docker push 406746673604.dkr.ecr.us-east-1.amazonaws.com/kusama-api:$COMMIT_HASH
```

### Initiate a rolling deployment

```
kubectl edit deployments.apps/kusama-api-deployment
```

It'll open up default editor. Look for the container repo and image name. Change the image tag and save & exit. It will start rolling deployment without causing any outage. It takes ~5mins to deploy to the two pods we have right now.
